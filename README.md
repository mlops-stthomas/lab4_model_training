# Lab 4: Model Training and Serving with Airflow + FastAPI

In this lab you will build an **end-to-end ML pipeline** using Apache Airflow and serve the trained model with FastAPI.  

The pipeline includes:  
1. **Generate Data** â€“ downloads the Iris dataset and saves it as a CSV.  
2. **Train Model** â€“ trains a Logistic Regression classifier.  
3. **Pipeline** â€“ runs both steps end-to-end.  
4. **Serve Model** â€“ starts a FastAPI app for inference.  

---

## ğŸ“‚ Project Structure

```
lab4_model_training/
â”œâ”€â”€ dags/                        # Airflow DAGs
â”‚   â”œâ”€â”€ ml_pipeline_dag.py       # full pipeline: generate + train
â”‚   â”œâ”€â”€ generate_data_dag.py     # generate dataset only
â”‚   â””â”€â”€ train_model_dag.py       # train model only
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ml_pipeline/             # training pipeline
â”‚   â”‚   â”œâ”€â”€ data.py
â”‚   â”‚   â””â”€â”€ model.py
â”‚   â””â”€â”€ app/                     # serving app
â”‚       â””â”€â”€ api.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py         # CLI wrapper
â”‚   â”œâ”€â”€ train_model.py           # CLI wrapper
â”‚   â””â”€â”€ serve_api.py             # runs FastAPI app
â”œâ”€â”€ data/                        # dataset outputs
â”‚   â””â”€â”€ iris.csv
â”œâ”€â”€ models/                      # trained models
â”‚   â””â”€â”€ iris_model.pkl
â”œâ”€â”€ airflow_home/                # Airflow metadata (created after setup)
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ setup_airflow.sh             # one-time setup script
```

---

## ğŸ›  Environment Setup

We use **one virtual environment** for all labs.

1. Create and activate:

```
python3 -m venv ~/venvs/airflow-class
source ~/venvs/airflow-class/bin/activate
```

2. Install dependencies:

```
pip install -r requirements.txt
```

âš ï¸ The `requirements.txt` pins **Airflow 2.10.2**. If you are not on Python 3.10, update the constraints line to match (`constraints-3.9.txt` or `constraints-3.11.txt`).  

---

## âš™ï¸ Airflow Setup (one time)

Run the setup script:

```
./setup_airflow.sh
```

This will:  
- Set `AIRFLOW_HOME` inside this project.  
- Initialize the Airflow database.  
- Create an admin user (`admin / admin`).  
- Symlink your `dags/` folder into Airflowâ€™s DAGs directory.  

Afterwards, open a new terminal (or `source ~/.bashrc` / `~/.zshrc`) so `$AIRFLOW_HOME` is available automatically.  

---

## ğŸš€ Running Airflow

Use two terminals:

**Terminal 1 â€“ Scheduler**
```
source ~/venvs/airflow-class/bin/activate
airflow scheduler
```

**Terminal 2 â€“ Webserver**
```
source ~/venvs/airflow-class/bin/activate
airflow webserver --port 8080 --host 0.0.0.0
```

Then visit ğŸ‘‰ http://<ipaddress>:8080  
Login: `admin / admin`

Replace `<ipaddress>` with your EC2 instanceâ€™s **public IPv4 address**.  

---

## ğŸ“Š DAGs to Explore

You will see three DAGs:

1. **`generate_data_only`**  
   - Saves `data/iris.csv`.

2. **`train_model_only`**  
   - Trains a Logistic Regression model from CSV.  
   - Produces `models/iris_model.pkl`.

3. **`ml_pipeline`**  
   - End-to-end pipeline:  
     `generate_data` â†’ `train_model`.

---

## ğŸ§ª Testing Without Airflow

You can also run scripts directly:

```
python scripts/generate_data.py
python scripts/train_model.py
```

This will produce `data/iris.csv` and `models/iris_model.pkl`.  

---

## ğŸŒ Serving the Model with FastAPI

After training the model, you can serve it with FastAPI.

1. Run the API:

```
python scripts/serve_api.py
```

2. Open docs: http://<ipaddress>:8000/docs  

3. Try a prediction in Swagger UI with four required features:

```
{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}
```

Response:

```
{"prediction": "setosa", "class_index": 0}
```

---

## ğŸŒ¸ Example Inputs

Values that commonly predict each class:

- **Setosa**: `5.1, 3.5, 1.4, 0.2`  
- **Versicolor**: `6.0, 2.9, 4.5, 1.5`  
- **Virginica**: `6.9, 3.1, 5.4, 2.1`  

---

## âœ… Summary

By the end of this lab you will have:  
- Built a training pipeline with Airflow.  
- Produced a dataset and a trained model artifact.  
- Served the trained model with FastAPI.  
- Sent live inference requests to your model.  

Next steps: containerize this API and deploy it to the cloud ğŸš€
